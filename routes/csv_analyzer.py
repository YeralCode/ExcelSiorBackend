"""
Ruta para el analizador interactivo de archivos CSV.

Proporciona endpoints para analizar archivos CSV y generar
diccionarios de tipos de datos autom√°ticamente.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import re
import os
import shutil
import tempfile

from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from utils.logger import get_logger
from utils.exceptions import handle_exception
from utils.dynamic_data_types import data_type_manager
from utils.csv_separator_detector import separator_detector
from utils.column_cleaner import column_cleaner
from repository.processors.csv_processor import CSVProcessor
from config.settings import get_project_config

logger = get_logger(__name__)
router = APIRouter(prefix="/csv-analyzer", tags=["CSV Analyzer"])

class CSVAnalysisRequest(BaseModel):
    """Modelo para solicitudes de an√°lisis de CSV."""
    project_name: Optional[str] = None
    include_metadata: bool = True
    sample_size: int = 1000
    auto_detect_types: bool = True
    use_dynamic_types: bool = True  # Nuevo campo para usar tipos din√°micos

class CSVAnalysisResponse(BaseModel):
    """Modelo para respuestas de an√°lisis de CSV."""
    success: bool
    file_info: Dict[str, Any]
    column_analysis: Dict[str, Any]
    data_types: Dict[str, List[str]]
    recommendations: Dict[str, Any]
    sample_data: List[Dict[str, Any]]
    validation_results: Dict[str, Any]
    available_types: List[Dict[str, Any]]  # Nuevo campo para tipos disponibles

def normalize_column_name(column_name: str) -> str:
    """Normaliza nombres de columnas reemplazando espacios y caracteres especiales."""
    column_name = column_name.strip().upper()
    replacements = [
        (' ', '_'), ('-', '_'), ('√Å', 'A'), ('√â', 'E'), ('√ç', 'I'),
        ('√ì', 'O'), ('√ö', 'U'), ('√ë', 'N'), ('.', ''), ('/', '_'),
    ]
    for old, new in replacements:
        column_name = column_name.replace(old, new)
    return column_name

class DataTypeDetector:
    """Detector autom√°tico de tipos de datos para columnas CSV."""
    
    def __init__(self, use_dynamic_types: bool = True):
        self.use_dynamic_types = use_dynamic_types
        self.null_values = {"", "nan", "null", "n/a", "none", "undefined"}
        self.date_patterns = [
            r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
            r'\d{2}/\d{2}/\d{4}',  # MM/DD/YYYY
            r'\d{2}-\d{2}-\d{4}',  # MM-DD-YYYY
            r'\d{4}/\d{2}/\d{2}',  # YYYY/MM/DD
            r'\d{2}/\d{2}/\d{2}',  # DD/MM/YY
            r'\d{2}-\d{2}-\d{2}',  # DD-MM-YY
        ]
        
    def detect_column_type(self, column_data: pd.Series, column_name: str) -> Dict[str, Any]:
        """
        Detecta el tipo de datos de una columna.
        
        Args:
            column_data: Serie de pandas con los datos de la columna
            column_name: Nombre de la columna
            
        Returns:
            Diccionario con informaci√≥n del tipo detectado
        """
        # Limpiar datos nulos
        clean_data = column_data.dropna()
        if len(clean_data) == 0:
            return {"type": "string", "confidence": 0.0, "reason": "Solo valores nulos"}
        
        # Convertir a string para an√°lisis
        str_data = clean_data.astype(str).str.strip()
        
        # Si se usan tipos din√°micos, intentar detectar primero
        if self.use_dynamic_types:
            dynamic_result = data_type_manager.detect_type(str_data.tolist(), column_name)
            if dynamic_result["confidence"] > 0.7:
                return dynamic_result
        
        # Detectar tipos b√°sicos
        return self._detect_basic_types(str_data, column_name)
    
    def _detect_basic_types(self, str_data: pd.Series, column_name: str) -> Dict[str, Any]:
        """Detecta tipos b√°sicos de datos."""
        
        # Detectar fechas
        if self._detect_date_type(str_data):
            return {"type": "date", "confidence": 0.8, "reason": "Patrones de fecha detectados"}
        
        # Detectar n√∫meros enteros
        if self._detect_integer_type(str_data):
            return {"type": "integer", "confidence": 0.9, "reason": "N√∫meros enteros detectados"}
        
        # Detectar n√∫meros decimales
        if self._detect_float_type(str_data):
            return {"type": "float", "confidence": 0.85, "reason": "N√∫meros decimales detectados"}
        
        # Detectar booleanos
        if self._detect_boolean_type(str_data):
            return {"type": "boolean", "confidence": 0.9, "reason": "Valores booleanos detectados"}
        
        # Por defecto, es string
        return {"type": "string", "confidence": 0.5, "reason": "Texto detectado"}
    
    def _detect_date_type(self, str_data: pd.Series) -> bool:
        """Detecta si la columna contiene fechas."""
        date_count = 0
        total_valid = 0
        
        for value in str_data:
            if value and str(value).strip():
                total_valid += 1
                for pattern in self.date_patterns:
                    if re.search(pattern, str(value)):
                        date_count += 1
                        break
        
        return total_valid > 0 and (date_count / total_valid) > 0.7
    
    def _detect_integer_type(self, str_data: pd.Series) -> bool:
        """Detecta si la columna contiene n√∫meros enteros."""
        int_count = 0
        total_valid = 0
        
        for value in str_data:
            if value and str(value).strip():
                total_valid += 1
                try:
                    int(str(value).replace(',', '').replace('.', ''))
                    int_count += 1
                except ValueError:
                    pass
        
        return total_valid > 0 and (int_count / total_valid) > 0.8
    
    def _detect_float_type(self, str_data: pd.Series) -> bool:
        """Detecta si la columna contiene n√∫meros decimales."""
        float_count = 0
        total_valid = 0
        
        for value in str_data:
            if value and str(value).strip():
                total_valid += 1
                try:
                    float(str(value).replace(',', ''))
                    float_count += 1
                except ValueError:
                    pass
        
        return total_valid > 0 and (float_count / total_valid) > 0.8
    
    def _detect_boolean_type(self, str_data: pd.Series) -> bool:
        """Detecta si la columna contiene valores booleanos."""
        boolean_values = {'true', 'false', 'yes', 'no', '1', '0', 'si', 'no', 'verdadero', 'falso'}
        bool_count = 0
        total_valid = 0
        
        for value in str_data:
            if value and str(value).strip():
                total_valid += 1
                if str(value).lower().strip() in boolean_values:
                    bool_count += 1
        
        return total_valid > 0 and (bool_count / total_valid) > 0.8

def extract_unique_values(column_data: pd.Series, max_values: Optional[int] = None) -> List[str]:
    """
    Extrae valores √∫nicos de una columna.
    
    Args:
        column_data: Serie de pandas con los datos de la columna
        max_values: M√°ximo n√∫mero de valores √∫nicos a retornar (None para sin l√≠mite)
        
    Returns:
        Lista de valores √∫nicos
    """
    try:
        # Limpiar datos nulos y vac√≠os
        clean_data = column_data.dropna()
        clean_data = clean_data[clean_data.astype(str).str.strip() != '']
        
        if len(clean_data) == 0:
            return []
        
        # Obtener valores √∫nicos
        unique_values = clean_data.astype(str).str.strip().unique()
        
        # Ordenar y limitar solo si se especifica un l√≠mite
        if max_values is not None and max_values > 0:
            unique_values = sorted(unique_values)[:max_values]
        else:
            # Sin l√≠mite - ordenar todos los valores √∫nicos
            unique_values = sorted(unique_values)
        
        # Convertir a lista de strings
        return [str(value) for value in unique_values]
    except Exception as e:
        logger.warning(f"Error extrayendo valores √∫nicos: {str(e)}")
        return []

@router.post("/analyze")
async def analyze_csv(
    file: UploadFile = File(...),
    use_dynamic_types: bool = Form(True),
    extract_choices: bool = Form(True)  # Nuevo par√°metro para extraer choices
):
    try:
        # Crear directorio temporal
        temp_dir = tempfile.mkdtemp()
        temp_path = os.path.join(temp_dir, file.filename)
        
        # Guardar archivo temporalmente
        with open(temp_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
        
        # Detectar separador autom√°ticamente
        separator_info = separator_detector.get_separator_info(temp_path)
        detected_separator = separator_info["detected_separator"]
        separator_confidence = separator_info["confidence"]
        
        logger.info(f"Separador detectado: '{detected_separator}' (confianza: {separator_confidence:.2f})")
        
        try:
            # Intentar leer con diferentes configuraciones usando el separador detectado
            df = None
            error_messages = []
            
            # Configuraci√≥n 1: Lectura b√°sica con separador detectado
            try:
                df = pd.read_csv(temp_path, dtype=str, encoding='utf-8', sep=detected_separator)
                logger.info(f"CSV le√≠do exitosamente con separador '{detected_separator}'")
            except Exception as e1:
                error_messages.append(f"Separador '{detected_separator}' fall√≥: {str(e1)}")
                
                # Configuraci√≥n 2: Con encoding latin-1
                try:
                    df = pd.read_csv(temp_path, dtype=str, encoding='latin-1', sep=detected_separator)
                    logger.info(f"CSV le√≠do exitosamente con separador '{detected_separator}' y encoding latin-1")
                except Exception as e2:
                    error_messages.append(f"Encoding latin-1 fall√≥: {str(e2)}")
                    
                    # Configuraci√≥n 3: Con encoding cp1252
                    try:
                        df = pd.read_csv(temp_path, dtype=str, encoding='cp1252', sep=detected_separator)
                        logger.info(f"CSV le√≠do exitosamente con separador '{detected_separator}' y encoding cp1252")
                    except Exception as e3:
                        error_messages.append(f"Encoding cp1252 fall√≥: {str(e3)}")
                        
                        # Configuraci√≥n 4: Con engine python
                        try:
                            df = pd.read_csv(temp_path, dtype=str, engine='python', sep=detected_separator)
                            logger.info(f"CSV le√≠do exitosamente con separador '{detected_separator}' y engine python")
                        except Exception as e4:
                            error_messages.append(f"Engine python fall√≥: {str(e4)}")
                            
                            # Configuraci√≥n 5: Ignorar l√≠neas problem√°ticas
                            try:
                                df = pd.read_csv(temp_path, dtype=str, on_bad_lines='skip', sep=detected_separator)
                                logger.info(f"CSV le√≠do exitosamente con separador '{detected_separator}' ignorando l√≠neas problem√°ticas")
                            except Exception as e5:
                                error_messages.append(f"Ignorar l√≠neas problem√°ticas fall√≥: {str(e5)}")
                                
                                # Configuraci√≥n 6: √öltimo intento con separador autom√°tico
                                try:
                                    df = pd.read_csv(temp_path, dtype=str, sep=None, engine='python')
                                    logger.info("CSV le√≠do exitosamente con separador autom√°tico")
                                except Exception as e6:
                                    error_messages.append(f"Separador autom√°tico fall√≥: {str(e6)}")
                                    raise Exception(f"No se pudo leer el archivo CSV. Errores: {'; '.join(error_messages)}")
            
            if df is None or df.empty:
                raise Exception("El archivo CSV est√° vac√≠o o no se pudo leer correctamente")
            
            # Limpiar columnas con nombres problem√°ticos
            df.columns = [str(col).strip() for col in df.columns]
            
            # Eliminar columnas completamente vac√≠as
            df = df.dropna(axis=1, how='all')
            
            # Si no hay columnas despu√©s de la limpieza
            if df.empty or len(df.columns) == 0:
                raise Exception("No se encontraron columnas v√°lidas en el archivo CSV")
            
            # Crear mapeo de nombres de columnas limpios
            column_mapping = column_cleaner.clean_column_names(list(df.columns))
            
            logger.info(f"CSV procesado exitosamente: {len(df)} filas, {len(df.columns)} columnas")
            logger.info(f"Mapeo de columnas: {column_mapping}")
            
        except Exception as e:
            logger.error(f"Error leyendo CSV: {str(e)}")
            raise HTTPException(status_code=400, detail=f"Error leyendo archivo CSV: {str(e)}")
        
        # Inicializar detector
        detector = DataTypeDetector(use_dynamic_types=use_dynamic_types)
        
        # Analizar cada columna
        column_analysis = {}
        column_choices = {}  # Nuevo diccionario para almacenar choices
        
        for column in df.columns:
            try:
                analysis = detector.detect_column_type(df[column], column)
                column_analysis[column] = analysis
                
                # Extraer valores √∫nicos si se solicita - SIN L√çMITE M√ÅXIMO
                if extract_choices:
                    unique_values = extract_unique_values(df[column], max_values=None)  # Sin l√≠mite
                    column_choices[column] = unique_values
                    logger.info(f"Columna '{column}': {len(unique_values)} valores √∫nicos extra√≠dos (sin l√≠mite)")
                
            except Exception as e:
                logger.warning(f"Error analizando columna {column}: {str(e)}")
                # Usar tipo por defecto si hay error
                column_analysis[column] = {
                    "type": "string",
                    "confidence": 0.0,
                    "reason": f"Error en an√°lisis: {str(e)}"
                }
                column_choices[column] = []
        
        # Normalizar nombres de columnas para el JSON de respuesta
        normalized_column_analysis = {}
        normalized_column_choices = {}
        
        for original_name, analysis in column_analysis.items():
            normalized_name = normalize_column_name(original_name)
            normalized_column_analysis[normalized_name] = analysis
            if extract_choices and original_name in column_choices:
                normalized_column_choices[normalized_name] = column_choices[original_name]
        
        # Informaci√≥n del archivo
        file_info = {
            "filename": file.filename,
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "sample_size": min(1000, len(df)),
            "detected_separator": detected_separator,
            "separator_confidence": separator_confidence,
            "column_mapping": column_mapping
        }
        
        # Obtener tipos disponibles si se usan tipos din√°micos
        available_types = []
        if use_dynamic_types:
            available_types = data_type_manager.get_all_types()
        
        # Preparar respuesta
        response_data = {
            "success": True,
            "file_info": file_info,
            "column_analysis": normalized_column_analysis,  # Usar nombres normalizados
            "column_choices": normalized_column_choices,  # Nuevo campo con choices
            "available_types": available_types,
            "separator_info": separator_info,
            "column_mapping": column_mapping,
            "original_columns": list(df.columns),  # Agregar nombres originales de columnas
            "normalized_columns": [normalize_column_name(col) for col in df.columns],  # Agregar nombres normalizados
            "recommendations": {
                "total_columns_analyzed": len(df.columns),
                "types_detected": len(set(analysis["type"] for analysis in column_analysis.values())),
                "use_dynamic_types": use_dynamic_types,
                "recommended_separator": detected_separator,
                "choices_extracted": extract_choices
            }
        }
        
        # Limpiar archivo temporal
        try:
            os.remove(temp_path)
            os.rmdir(temp_dir)
        except:
            pass  # Ignorar errores de limpieza
        
        logger.info(f"An√°lisis completado para {file.filename}")
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error analizando CSV: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error analizando archivo: {str(e)}")

@router.post("/get-column-choices")
async def get_column_choices(
    file: UploadFile = File(...),
    column_name: str = Form(...),
    max_values: Optional[int] = Form(None)
):
    try:
        # Crear directorio temporal
        temp_dir = tempfile.mkdtemp()
        temp_path = os.path.join(temp_dir, file.filename)
        
        # Guardar archivo temporalmente
        with open(temp_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
        
        # Verificar que el archivo se guard√≥ correctamente
        if os.path.exists(temp_path):
            file_size = os.path.getsize(temp_path)
            logger.info(f"   ‚úÖ Archivo guardado. Tama√±o en disco: {file_size} bytes")
        else:
            raise Exception("El archivo temporal no se cre√≥ correctamente")
        
        # Detectar separador autom√°ticamente
        separator_info = separator_detector.get_separator_info(temp_path)
        detected_separator = separator_info["detected_separator"]
        
        # Leer CSV
        try:
            df = pd.read_csv(temp_path, dtype=str, encoding='utf-8', sep=detected_separator)
            logger.info(f"   ‚úÖ CSV le√≠do exitosamente con separador '{detected_separator}' y encoding utf-8")
        except Exception as e1:
            logger.warning(f"   ‚ö†Ô∏è  Separador '{detected_separator}' con utf-8 fall√≥: {str(e1)}")
            
            # Configuraci√≥n 2: Con encoding latin-1
            try:
                df = pd.read_csv(temp_path, dtype=str, encoding='latin-1', sep=detected_separator)
            except Exception as e2:
                # Configuraci√≥n 3: Con engine python
                try:
                    df = pd.read_csv(temp_path, dtype=str, engine='python', sep=detected_separator)
                except Exception as e3:
                    # Configuraci√≥n 4: √öltimo intento con separador autom√°tico
                    try:
                        df = pd.read_csv(temp_path, dtype=str, sep=None, engine='python')
                    except Exception as e4:
                        raise Exception(f"No se pudo leer el archivo CSV: {str(e4)}")
        
        # Verificar que se ley√≥ todo el archivo
        logger.info(f"   üìä Archivo le√≠do completamente:")
        logger.info(f"      - Filas totales: {len(df):,}")
        logger.info(f"      - Columnas totales: {len(df.columns)}")
        logger.info(f"      - Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
        
        # üîç VERIFICACI√ìN ADICIONAL: Comparar con tama√±o del archivo
        file_size_bytes = os.path.getsize(temp_path)
        file_size_mb = file_size_bytes / 1024 / 1024
        logger.info(f"      - Tama√±o del archivo en disco: {file_size_mb:.2f} MB")
        
        # Calcular densidad de datos esperada
        if len(df) > 0:
            bytes_per_row = file_size_bytes / len(df)
            logger.info(f"      - Bytes por fila: {bytes_per_row:.2f}")
            logger.info(f"      - Densidad de datos: {len(df.columns) * bytes_per_row:.2f} bytes por fila-columna")
            
            # Verificar si la densidad parece razonable
            if bytes_per_row < 10:  # Menos de 10 bytes por fila es sospechoso
                logger.warning(f"      ‚ö†Ô∏è  ADVERTENCIA: Densidad de datos muy baja ({bytes_per_row:.2f} bytes/fila)")
                logger.warning(f"      ‚ö†Ô∏è  El archivo puede no haberse le√≠do completamente")
            elif bytes_per_row > 10000:  # M√°s de 10KB por fila es sospechoso
                logger.warning(f"      ‚ö†Ô∏è  ADVERTENCIA: Densidad de datos muy alta ({bytes_per_row:.2f} bytes/fila)")
                logger.warning(f"      ‚ö†Ô∏è  Verificar separador y encoding")
            else:
                logger.info(f"      ‚úÖ Densidad de datos parece normal")
        
        # üîç VERIFICACI√ìN ADICIONAL: Verificar que no haya truncamiento
        if len(df) > 0:
            logger.info(f"      - Primera fila: {df.iloc[0].tolist()[:3]}...")  # Primeras 3 columnas
            logger.info(f"      - √öltima fila: {df.iloc[-1].tolist()[:3]}...")  # Primeras 3 columnas
            
            # Verificar que la √∫ltima fila no est√© truncada
            last_row = df.iloc[-1]
            if last_row.isna().sum() > len(last_row) * 0.5:  # Si m√°s del 50% son NaN
                logger.warning(f"      ‚ö†Ô∏è  ADVERTENCIA: √öltima fila parece estar truncada")
                logger.warning(f"      ‚ö†Ô∏è  Verificar separador y encoding del archivo")
        
        # üîç VERIFICACI√ìN ADICIONAL: Intentar lectura alternativa si hay sospechas
        if file_size_mb > 100 and len(df) < 500000:  # Archivo grande con pocas filas
            logger.warning(f"      ‚ö†Ô∏è  SOSPECHA: Archivo muy grande ({file_size_mb:.2f} MB) pero pocas filas ({len(df):,})")
            logger.info(f"      üîç Intentando lectura alternativa para verificar...")
            
            try:
                # Intentar lectura con diferentes configuraciones
                df_alt1 = pd.read_csv(temp_path, dtype=str, sep=None, engine='python')
                logger.info(f"      - Lectura alternativa 1: {len(df_alt1):,} filas")
                
                df_alt2 = pd.read_csv(temp_path, dtype=str, sep=None, engine='c')
                logger.info(f"      - Lectura alternativa 2: {len(df_alt2):,} filas")
                
                # Usar el que tenga m√°s filas
                if len(df_alt1) > len(df) or len(df_alt2) > len(df):
                    if len(df_alt1) > len(df_alt2):
                        df = df_alt1
                        logger.info(f"      ‚úÖ Usando lectura alternativa 1: {len(df):,} filas")
                    else:
                        df = df_alt2
                        logger.info(f"      ‚úÖ Usando lectura alternativa 2: {len(df):,} filas")
                else:
                    logger.info(f"      ‚úÖ Lectura original es la mejor: {len(df):,} filas")
                    
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è  No se pudo realizar lectura alternativa: {str(e)}")
        
        # üîç VERIFICACI√ìN FINAL: Confirmar resultado
        logger.info(f"      üéØ RESULTADO FINAL:")
        logger.info(f"         - Filas procesadas: {len(df):,}")
        logger.info(f"         - Columnas procesadas: {len(df.columns)}")
        logger.info(f"         - Memoria final: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
        
        if len(df) == 0:
            raise Exception("El archivo CSV no contiene datos o no se pudo leer correctamente")
        
        # Crear mapeo de nombres normalizados a nombres originales
        normalized_to_original = {}
        for original_col in df.columns:
            normalized_col = normalize_column_name(original_col)
            normalized_to_original[normalized_col] = original_col
        
        # Buscar la columna (puede ser nombre original o normalizado)
        target_column = None
        
        # Primero buscar por nombre original
        if column_name in df.columns:
            target_column = column_name
            logger.info(f"Columna encontrada por nombre original: {column_name}")
        # Luego buscar por nombre normalizado
        elif column_name in normalized_to_original:
            target_column = normalized_to_original[column_name]
            logger.info(f"Columna encontrada por nombre normalizado: {column_name} -> {target_column}")
        else:
            # Si no se encuentra, mostrar todas las columnas disponibles para debugging
            available_columns = list(df.columns)
            available_normalized = [normalize_column_name(col) for col in df.columns]
            logger.error(f"   Columna '{column_name}' no encontrada. Columnas disponibles:")
            logger.error(f"      Originales: {available_columns}")
            logger.error(f"      Normalizadas: {available_normalized}")
            raise HTTPException(
                status_code=400, 
                detail=f"La columna '{column_name}' no existe en el archivo. Columnas disponibles: {available_columns}"
            )
        
        # Verificar datos de la columna objetivo
        column_data = df[target_column]
        
        # Extraer valores √∫nicos - SIN L√çMITE por defecto
        logger.info(f"   üîç Extrayendo valores √∫nicos para columna '{target_column}'")
        logger.info(f"   üîç Par√°metro max_values recibido: {max_values} (tipo: {type(max_values)})")
        
        unique_values = extract_unique_values(column_data, max_values=max_values)
        
        logger.info(f"   ‚úÖ Valores √∫nicos extra√≠dos exitosamente:")
        logger.info(f"      - Total extra√≠do: {len(unique_values)}")
        logger.info(f"      - Primeros 5 valores: {unique_values[:5] if unique_values else 'Ninguno'}")
        logger.info(f"      - √öltimos 5 valores: {unique_values[-5:] if len(unique_values) > 5 else unique_values}")
        
        # Limpiar archivo temporal
        try:
            os.remove(temp_path)
            os.rmdir(temp_dir)
        except:
            pass
        
        return {
            "success": True,
            "column_name": target_column,  # Nombre original de la columna
            "normalized_column_name": normalize_column_name(target_column),
            "unique_values": unique_values,
            "total_unique_values": len(unique_values),
            "max_values_requested": max_values if max_values else "Sin l√≠mite",
            "total_rows_processed": len(df),
            "file_size_bytes": file.size,
            "separator_used": detected_separator
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error extrayendo choices de columna: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error extrayendo choices: {str(e)}")

@router.post("/detect-separator")
async def detect_csv_separator(file: UploadFile = File(...)):
    try:
        # Crear directorio temporal
        temp_dir = tempfile.mkdtemp()
        temp_path = os.path.join(temp_dir, file.filename)
        
        # Guardar archivo temporalmente
        with open(temp_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
        
        # Detectar separador
        separator_info = separator_detector.get_separator_info(temp_path)
        
        # Limpiar archivo temporal
        try:
            os.remove(temp_path)
            os.rmdir(temp_dir)
        except:
            pass
        
        return {
            "success": True,
            "separator_info": separator_info
        }
        
    except Exception as e:
        logger.error(f"Error detectando separador: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error detectando separador: {str(e)}")

@router.get("/generate-type-dictionary")
async def generate_type_dictionary():
    """Genera un diccionario de tipos de datos basado en los tipos disponibles."""
    try:
        types = data_type_manager.get_all_types()
        
        # Crear diccionario de tipos
        type_dictionary = {}
        for type_info in types:
            type_dictionary[type_info['name']] = {
                "description": type_info['description'],
                "pattern": type_info['pattern'],
                "examples": type_info['examples'],
                "confidence_threshold": type_info['confidence_threshold']
            }
        
        return {
            "success": True,
            "type_dictionary": type_dictionary,
            "total_types": len(types)
        }
        
    except Exception as e:
        logger.error(f"Error generando diccionario: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error generando diccionario: {str(e)}") 